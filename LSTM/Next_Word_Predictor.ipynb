{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "FpGq-oGOIQbO"
      },
      "outputs": [],
      "source": [
        "data = \"\"\"Daily Conversations\n",
        "What time are you coming home? I’ll be home by 7 PM.\n",
        "Did you take out the trash? Not yet, I’ll do it in a few minutes.\n",
        "How was your day at work? It was good, but a little hectic.\n",
        "What do you want for dinner? I’m craving pasta today.\n",
        "Can you pick up some milk on your way home? Sure, I’ll stop by the store.\n",
        "Did you call the plumber? Yes, he’s coming tomorrow at 10 AM.\n",
        "What’s the weather like today? It’s sunny but a bit windy.\n",
        "Where did you put my keys? They’re on the kitchen counter.\n",
        "Have you seen my phone? Check the living room, I think you left it there.\n",
        "Are we still meeting at 5? Yes, I’ll see you at the café.\n",
        "Do you need anything from the supermarket? Yes, please get some eggs and bread.\n",
        "How’s your mom doing? She’s doing well, thanks for asking.\n",
        "Let me know when you reach home. Okay, I’ll text you when I get there.\n",
        "What’s the plan for the weekend? Maybe a movie or a short trip.\n",
        "Can you lower the volume a bit? Sure, I’ll turn it down.\n",
        "Is the coffee machine working? Yes, but you need to refill the water.\n",
        "Did you lock the front door? Yes, I checked twice.\n",
        "I’m running late, start without me. No worries, I’ll save you a seat.\n",
        "Want to go for a walk after dinner? That sounds great! Let’s go.\n",
        "Do you have an extra charger? Yeah, check my bag.\n",
        "Can you send me the report? Sure, I’ll email it in a minute.\n",
        "Is the WiFi working? No, I think the router needs a restart.\n",
        "How long will it take to get there? About 30 minutes with traffic.\n",
        "Did you water the plants? Not yet, I’ll do it now.\n",
        "Are you free this evening? No, I have a meeting.\n",
        "What time is your flight? It’s at 9:30 AM.\n",
        "How do I turn on the TV? Press the power button on the remote.\n",
        "Do you want tea or coffee? I’ll have coffee, please.\n",
        "Let’s order pizza tonight. Sounds good!\n",
        "Could you help me carry these bags? Of course!\n",
        "Where’s the nearest ATM? It’s two blocks down the street.\n",
        "Did you hear about the new movie? Yes, I want to watch it this weekend.\n",
        "Can I borrow your umbrella? Sure, just bring it back later.\n",
        "Is the restaurant open today? Yes, but only until 10 PM.\n",
        "Have you finished your homework? Almost, just one question left.\n",
        "Do we have any sugar left? No, we need to buy some.\n",
        "Is there a pharmacy nearby? Yes, just around the corner.\n",
        "Did you book the tickets? Yes, I got them yesterday.\n",
        "Let’s go for ice cream. That’s a great idea!\n",
        "Where should we meet? Let’s meet at the mall entrance.\n",
        "What time is your appointment? It’s at 3 PM.\n",
        "Do you know the WiFi password? Check the back of the router.\n",
        "Did you lock the car? Yes, I double-checked.\n",
        "How much does this cost? It’s $20.\n",
        "Do you take credit cards? Yes, we accept all major cards.\n",
        "What’s your favorite book? I love reading mystery novels.\n",
        "Have you ever traveled abroad? Yes, I went to Italy last year.\n",
        "Are you feeling okay? Yeah, just a little tired.\n",
        "Can you turn on the lights? Sure, let me get the switch.\n",
        "What time does the train arrive? It should be here by 5:15 PM.\n",
        "Do you want to go hiking this weekend? That sounds fun! Let’s plan it.\n",
        "How do you cook rice properly? Use one cup of rice and two cups of water.\n",
        "Did you see the news today? No, what happened?\n",
        "I think I lost my wallet. Check your pockets again.\n",
        "What should I wear to the party? Something casual but nice.\n",
        "Do you need help with your luggage? No, I got it, thanks.\n",
        "When is the deadline for the project? It’s due next Friday.\n",
        "Can I get a taxi from here? Yes, there’s a stand down the street.\n",
        "Do you want to watch a movie tonight? Sure, let’s pick one on Netflix.\n",
        "How do I get to the nearest metro station? Walk straight for two blocks, then turn left.\n",
        "What time does the store close? At 9 PM.\n",
        "Have you ever tried sushi? Yes, and I love it!\n",
        "Do you know how to fix a flat tire? Yes, I can show you.\n",
        "How long have you been working here? About three years.\n",
        "Where is the best coffee shop around here? Try the one on 5th Avenue, it’s great.\n",
        "Can you help me move this table? Sure, let’s do it.\n",
        "Did you send the email? Yes, I sent it this morning.\n",
        "What are your plans for the holidays? I’m visiting my family.\n",
        "Do you know how to swim? Yes, I learned as a kid.\n",
        "Are you allergic to anything? No, I don’t have any allergies.\n",
        "What kind of music do you like? Mostly rock and jazz.\n",
        "Have you ever been to New York? No, but I’d love to visit someday.\n",
        "What’s your favorite hobby? I enjoy painting and reading.\n",
        "Can you recommend a good book? Yes, try “The Alchemist.”\n",
        "Where’s the nearest gas station? About a mile down the road.\n",
        "Do you prefer cats or dogs? I like both, but I have a dog.\n",
        "What do you usually have for breakfast? Toast and eggs.\n",
        "How do you make a cup of tea? Boil water, add tea leaves, and let it steep.\n",
        "Did you see the sunset today? Yes, it was beautiful.\n",
        "Are you a morning person? Not really, I prefer nights.\n",
        "Do you want to split the bill? Sure, let’s divide it equally.\n",
        "Why is traffic so bad today? There was an accident earlier.\n",
        "Have you been to this restaurant before? No, but I heard it’s good.\n",
        "What’s your dream job? Something creative, like writing or designing.\n",
        "How do I use this washing machine? Just press the start button after loading.\n",
        "What do you do in your free time? I like playing video games.\n",
        "Where did you grow up? In a small town near the mountains.\n",
        "Can you help me find my glasses? Sure, when did you last see them?\n",
        "How do I get a taxi in this city? You can use a ride-hailing app.\n",
        "What’s the best way to learn a new language? Practice speaking every day.\n",
        "Did you hear that new song? Yes, it’s stuck in my head!\n",
        "What’s your favorite season? I love autumn the most.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sLpekiEfQt-q"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts([data]) # this can take multiple inputs so its an arr."
      ],
      "metadata": {
        "id": "WRMubu7ZQ04C"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYY_SzHQRFUv",
        "outputId": "510a4166-3669-46c0-f710-eb10c854762b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'you': 1,\n",
              " 'the': 2,\n",
              " 'i': 3,\n",
              " 'a': 4,\n",
              " 'do': 5,\n",
              " 'yes': 6,\n",
              " 'it': 7,\n",
              " 'to': 8,\n",
              " 'your': 9,\n",
              " 'did': 10,\n",
              " 'have': 11,\n",
              " 'what': 12,\n",
              " 'how': 13,\n",
              " 'can': 14,\n",
              " 'i’ll': 15,\n",
              " 'for': 16,\n",
              " 'sure': 17,\n",
              " 'it’s': 18,\n",
              " 'is': 19,\n",
              " 'no': 20,\n",
              " 'this': 21,\n",
              " 'at': 22,\n",
              " 'but': 23,\n",
              " 'let’s': 24,\n",
              " 'are': 25,\n",
              " 'want': 26,\n",
              " 'on': 27,\n",
              " 'what’s': 28,\n",
              " 'my': 29,\n",
              " 'get': 30,\n",
              " 'and': 31,\n",
              " 'me': 32,\n",
              " 'time': 33,\n",
              " 'in': 34,\n",
              " 'today': 35,\n",
              " 'of': 36,\n",
              " 'pm': 37,\n",
              " 'like': 38,\n",
              " 'there': 39,\n",
              " 'we': 40,\n",
              " 'just': 41,\n",
              " 'home': 42,\n",
              " 'was': 43,\n",
              " 'good': 44,\n",
              " 'where': 45,\n",
              " 'check': 46,\n",
              " 'left': 47,\n",
              " 'see': 48,\n",
              " 'need': 49,\n",
              " 'know': 50,\n",
              " 'when': 51,\n",
              " 'or': 52,\n",
              " 'turn': 53,\n",
              " 'down': 54,\n",
              " 'coffee': 55,\n",
              " 'water': 56,\n",
              " 'go': 57,\n",
              " 'about': 58,\n",
              " 'help': 59,\n",
              " 'new': 60,\n",
              " 'one': 61,\n",
              " 'love': 62,\n",
              " 'here': 63,\n",
              " 'by': 64,\n",
              " 'take': 65,\n",
              " 'not': 66,\n",
              " 'i’m': 67,\n",
              " 'some': 68,\n",
              " 'think': 69,\n",
              " 'let': 70,\n",
              " 'weekend': 71,\n",
              " 'movie': 72,\n",
              " 'working': 73,\n",
              " 'that': 74,\n",
              " 'sounds': 75,\n",
              " 'great': 76,\n",
              " 'tea': 77,\n",
              " 'nearest': 78,\n",
              " 'two': 79,\n",
              " 'book': 80,\n",
              " 'should': 81,\n",
              " 'does': 82,\n",
              " 'favorite': 83,\n",
              " 'ever': 84,\n",
              " 'use': 85,\n",
              " 'been': 86,\n",
              " 'coming': 87,\n",
              " 'be': 88,\n",
              " 'yet': 89,\n",
              " 'minutes': 90,\n",
              " 'day': 91,\n",
              " 'little': 92,\n",
              " 'dinner': 93,\n",
              " 'pick': 94,\n",
              " 'up': 95,\n",
              " 'way': 96,\n",
              " 'store': 97,\n",
              " '10': 98,\n",
              " 'am': 99,\n",
              " 'bit': 100,\n",
              " 'meeting': 101,\n",
              " '5': 102,\n",
              " 'anything': 103,\n",
              " 'from': 104,\n",
              " 'please': 105,\n",
              " 'eggs': 106,\n",
              " 'doing': 107,\n",
              " 'thanks': 108,\n",
              " 'okay': 109,\n",
              " 'plan': 110,\n",
              " 'machine': 111,\n",
              " 'lock': 112,\n",
              " 'checked': 113,\n",
              " 'start': 114,\n",
              " 'walk': 115,\n",
              " 'after': 116,\n",
              " 'an': 117,\n",
              " 'yeah': 118,\n",
              " 'send': 119,\n",
              " 'email': 120,\n",
              " 'wifi': 121,\n",
              " 'router': 122,\n",
              " 'long': 123,\n",
              " '30': 124,\n",
              " 'with': 125,\n",
              " 'traffic': 126,\n",
              " 'free': 127,\n",
              " '9': 128,\n",
              " 'press': 129,\n",
              " 'button': 130,\n",
              " 'tonight': 131,\n",
              " 'where’s': 132,\n",
              " 'blocks': 133,\n",
              " 'street': 134,\n",
              " 'hear': 135,\n",
              " 'watch': 136,\n",
              " 'back': 137,\n",
              " 'restaurant': 138,\n",
              " 'any': 139,\n",
              " 'around': 140,\n",
              " 'got': 141,\n",
              " 'them': 142,\n",
              " 'meet': 143,\n",
              " 'cards': 144,\n",
              " 'reading': 145,\n",
              " 'last': 146,\n",
              " 'rice': 147,\n",
              " 'cup': 148,\n",
              " 'something': 149,\n",
              " 'taxi': 150,\n",
              " 'station': 151,\n",
              " 'best': 152,\n",
              " 'try': 153,\n",
              " 'morning': 154,\n",
              " 'prefer': 155,\n",
              " 'daily': 156,\n",
              " 'conversations': 157,\n",
              " '7': 158,\n",
              " 'out': 159,\n",
              " 'trash': 160,\n",
              " 'few': 161,\n",
              " 'work': 162,\n",
              " 'hectic': 163,\n",
              " 'craving': 164,\n",
              " 'pasta': 165,\n",
              " 'milk': 166,\n",
              " 'stop': 167,\n",
              " 'call': 168,\n",
              " 'plumber': 169,\n",
              " 'he’s': 170,\n",
              " 'tomorrow': 171,\n",
              " 'weather': 172,\n",
              " 'sunny': 173,\n",
              " 'windy': 174,\n",
              " 'put': 175,\n",
              " 'keys': 176,\n",
              " 'they’re': 177,\n",
              " 'kitchen': 178,\n",
              " 'counter': 179,\n",
              " 'seen': 180,\n",
              " 'phone': 181,\n",
              " 'living': 182,\n",
              " 'room': 183,\n",
              " 'still': 184,\n",
              " 'café': 185,\n",
              " 'supermarket': 186,\n",
              " 'bread': 187,\n",
              " 'how’s': 188,\n",
              " 'mom': 189,\n",
              " 'she’s': 190,\n",
              " 'well': 191,\n",
              " 'asking': 192,\n",
              " 'reach': 193,\n",
              " 'text': 194,\n",
              " 'maybe': 195,\n",
              " 'short': 196,\n",
              " 'trip': 197,\n",
              " 'lower': 198,\n",
              " 'volume': 199,\n",
              " 'refill': 200,\n",
              " 'front': 201,\n",
              " 'door': 202,\n",
              " 'twice': 203,\n",
              " 'running': 204,\n",
              " 'late': 205,\n",
              " 'without': 206,\n",
              " 'worries': 207,\n",
              " 'save': 208,\n",
              " 'seat': 209,\n",
              " 'extra': 210,\n",
              " 'charger': 211,\n",
              " 'bag': 212,\n",
              " 'report': 213,\n",
              " 'minute': 214,\n",
              " 'needs': 215,\n",
              " 'restart': 216,\n",
              " 'will': 217,\n",
              " 'plants': 218,\n",
              " 'now': 219,\n",
              " 'evening': 220,\n",
              " 'flight': 221,\n",
              " 'tv': 222,\n",
              " 'power': 223,\n",
              " 'remote': 224,\n",
              " 'order': 225,\n",
              " 'pizza': 226,\n",
              " 'could': 227,\n",
              " 'carry': 228,\n",
              " 'these': 229,\n",
              " 'bags': 230,\n",
              " 'course': 231,\n",
              " 'atm': 232,\n",
              " 'borrow': 233,\n",
              " 'umbrella': 234,\n",
              " 'bring': 235,\n",
              " 'later': 236,\n",
              " 'open': 237,\n",
              " 'only': 238,\n",
              " 'until': 239,\n",
              " 'finished': 240,\n",
              " 'homework': 241,\n",
              " 'almost': 242,\n",
              " 'question': 243,\n",
              " 'sugar': 244,\n",
              " 'buy': 245,\n",
              " 'pharmacy': 246,\n",
              " 'nearby': 247,\n",
              " 'corner': 248,\n",
              " 'tickets': 249,\n",
              " 'yesterday': 250,\n",
              " 'ice': 251,\n",
              " 'cream': 252,\n",
              " 'that’s': 253,\n",
              " 'idea': 254,\n",
              " 'mall': 255,\n",
              " 'entrance': 256,\n",
              " 'appointment': 257,\n",
              " '3': 258,\n",
              " 'password': 259,\n",
              " 'car': 260,\n",
              " 'double': 261,\n",
              " 'much': 262,\n",
              " 'cost': 263,\n",
              " '20': 264,\n",
              " 'credit': 265,\n",
              " 'accept': 266,\n",
              " 'all': 267,\n",
              " 'major': 268,\n",
              " 'mystery': 269,\n",
              " 'novels': 270,\n",
              " 'traveled': 271,\n",
              " 'abroad': 272,\n",
              " 'went': 273,\n",
              " 'italy': 274,\n",
              " 'year': 275,\n",
              " 'feeling': 276,\n",
              " 'tired': 277,\n",
              " 'lights': 278,\n",
              " 'switch': 279,\n",
              " 'train': 280,\n",
              " 'arrive': 281,\n",
              " '15': 282,\n",
              " 'hiking': 283,\n",
              " 'fun': 284,\n",
              " 'cook': 285,\n",
              " 'properly': 286,\n",
              " 'cups': 287,\n",
              " 'news': 288,\n",
              " 'happened': 289,\n",
              " 'lost': 290,\n",
              " 'wallet': 291,\n",
              " 'pockets': 292,\n",
              " 'again': 293,\n",
              " 'wear': 294,\n",
              " 'party': 295,\n",
              " 'casual': 296,\n",
              " 'nice': 297,\n",
              " 'luggage': 298,\n",
              " 'deadline': 299,\n",
              " 'project': 300,\n",
              " 'due': 301,\n",
              " 'next': 302,\n",
              " 'friday': 303,\n",
              " 'there’s': 304,\n",
              " 'stand': 305,\n",
              " 'netflix': 306,\n",
              " 'metro': 307,\n",
              " 'straight': 308,\n",
              " 'then': 309,\n",
              " 'close': 310,\n",
              " 'tried': 311,\n",
              " 'sushi': 312,\n",
              " 'fix': 313,\n",
              " 'flat': 314,\n",
              " 'tire': 315,\n",
              " 'show': 316,\n",
              " 'three': 317,\n",
              " 'years': 318,\n",
              " 'shop': 319,\n",
              " '5th': 320,\n",
              " 'avenue': 321,\n",
              " 'move': 322,\n",
              " 'table': 323,\n",
              " 'sent': 324,\n",
              " 'plans': 325,\n",
              " 'holidays': 326,\n",
              " 'visiting': 327,\n",
              " 'family': 328,\n",
              " 'swim': 329,\n",
              " 'learned': 330,\n",
              " 'as': 331,\n",
              " 'kid': 332,\n",
              " 'allergic': 333,\n",
              " 'don’t': 334,\n",
              " 'allergies': 335,\n",
              " 'kind': 336,\n",
              " 'music': 337,\n",
              " 'mostly': 338,\n",
              " 'rock': 339,\n",
              " 'jazz': 340,\n",
              " 'york': 341,\n",
              " 'i’d': 342,\n",
              " 'visit': 343,\n",
              " 'someday': 344,\n",
              " 'hobby': 345,\n",
              " 'enjoy': 346,\n",
              " 'painting': 347,\n",
              " 'recommend': 348,\n",
              " '“the': 349,\n",
              " 'alchemist': 350,\n",
              " '”': 351,\n",
              " 'gas': 352,\n",
              " 'mile': 353,\n",
              " 'road': 354,\n",
              " 'cats': 355,\n",
              " 'dogs': 356,\n",
              " 'both': 357,\n",
              " 'dog': 358,\n",
              " 'usually': 359,\n",
              " 'breakfast': 360,\n",
              " 'toast': 361,\n",
              " 'make': 362,\n",
              " 'boil': 363,\n",
              " 'add': 364,\n",
              " 'leaves': 365,\n",
              " 'steep': 366,\n",
              " 'sunset': 367,\n",
              " 'beautiful': 368,\n",
              " 'person': 369,\n",
              " 'really': 370,\n",
              " 'nights': 371,\n",
              " 'split': 372,\n",
              " 'bill': 373,\n",
              " 'divide': 374,\n",
              " 'equally': 375,\n",
              " 'why': 376,\n",
              " 'so': 377,\n",
              " 'bad': 378,\n",
              " 'accident': 379,\n",
              " 'earlier': 380,\n",
              " 'before': 381,\n",
              " 'heard': 382,\n",
              " 'dream': 383,\n",
              " 'job': 384,\n",
              " 'creative': 385,\n",
              " 'writing': 386,\n",
              " 'designing': 387,\n",
              " 'washing': 388,\n",
              " 'loading': 389,\n",
              " 'playing': 390,\n",
              " 'video': 391,\n",
              " 'games': 392,\n",
              " 'grow': 393,\n",
              " 'small': 394,\n",
              " 'town': 395,\n",
              " 'near': 396,\n",
              " 'mountains': 397,\n",
              " 'find': 398,\n",
              " 'glasses': 399,\n",
              " 'city': 400,\n",
              " 'ride': 401,\n",
              " 'hailing': 402,\n",
              " 'app': 403,\n",
              " 'learn': 404,\n",
              " 'language': 405,\n",
              " 'practice': 406,\n",
              " 'speaking': 407,\n",
              " 'every': 408,\n",
              " 'song': 409,\n",
              " 'stuck': 410,\n",
              " 'head': 411,\n",
              " 'season': 412,\n",
              " 'autumn': 413,\n",
              " 'most': 414}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we are checking all the sentences by separating them on the basis on a nextline\n",
        "# tokenizing the sentences into a mapped numeric vectors.\n",
        "# then we will output map each sentence vector, this way we will form a supervised dataset\n",
        "\n",
        "input_sequences = []\n",
        "\n",
        "count = 1\n",
        "for sentence in data.split(\"\\n\"):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1, len(tokenized_sentence)):\n",
        "    n_gram = tokenized_sentence[:i+1]\n",
        "    input_sequences.append(n_gram)\n",
        "\n",
        "# input_sequences"
      ],
      "metadata": {
        "id": "UM1nzb74RJpG"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making every input shape is the same, so adding padding\n",
        "\n",
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "7EARz-M6UBwn"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding=\"pre\") # adding pre padding."
      ],
      "metadata": {
        "id": "nm5CWQKB192F"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:, :-1] # taking all input except the last element.\n",
        "y = padded_input_sequences[:, -1] # only keeping the last element from each vector sequences."
      ],
      "metadata": {
        "id": "nq_XjeRA2QiC"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS7ZRVFj2s0r",
        "outputId": "45e53a17-43d6-4bc2-c429-301614aa0c5f"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAe70IXK4UQR",
        "outputId": "47c57d69-cb7f-4258-c655-a999ce4b7622"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989,)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "49aLh9q5GkLX",
        "outputId": "1b31f83b-0dac-4ab6-c6cb-d0b6c832811d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# to find out the number of classses, we can just take the len of tokenizer which\n",
        "# uniquely maps each word to a number, that many words are our classes. make sure to add + 1 the len\n",
        "total_classes = len(tokenizer.word_index) + 1\n",
        "\n",
        "y = to_categorical(y , num_classes=total_classes)"
      ],
      "metadata": {
        "id": "-yS_Dd-j4Vty"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtxA9yHV4djr",
        "outputId": "c3c5fc37-d97b-4b72-8b2e-06ab2dfe7b3b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989, 415)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "tA_FhIqE62zH"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "id": "T94R-gNKCOdI",
        "outputId": "fd7f8c78-21a0-4e5b-a763-caf863e269fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "989"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "id": "53NMxAPECQ9f",
        "outputId": "abd505b7-6c16-4978-b291-104de8675c2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "989"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will have 3 layers, first embedding, then the LSTM and then a Dense with softmax\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_classes, 100))\n",
        "model.add(LSTM(150)) # remember all that NN in the LSTM architecture, those with sigmoid, tanh NN\n",
        "model.add(Dense(total_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "vFR2uYfB9PYx",
        "outputId": "95d24a66-e9cb-46ba-87e4-8a36d895a511"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4ffTvbo9u3E",
        "outputId": "83725f65-9adb-4a76-8290-334da7d5d2ce"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.0244 - loss: 5.9411\n",
            "Epoch 2/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0638 - loss: 5.4453\n",
            "Epoch 3/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0866 - loss: 5.2607\n",
            "Epoch 4/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1012 - loss: 5.2310\n",
            "Epoch 5/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0855 - loss: 5.1079\n",
            "Epoch 6/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0876 - loss: 4.9644\n",
            "Epoch 7/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1172 - loss: 4.6756\n",
            "Epoch 8/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1270 - loss: 4.5267\n",
            "Epoch 9/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1257 - loss: 4.4762\n",
            "Epoch 10/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1533 - loss: 4.2203\n",
            "Epoch 11/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1759 - loss: 3.9995\n",
            "Epoch 12/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1711 - loss: 3.9152\n",
            "Epoch 13/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2037 - loss: 3.7227\n",
            "Epoch 14/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2082 - loss: 3.5933\n",
            "Epoch 15/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2382 - loss: 3.4382\n",
            "Epoch 16/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2493 - loss: 3.2859\n",
            "Epoch 17/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2922 - loss: 3.1068\n",
            "Epoch 18/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3145 - loss: 2.9888\n",
            "Epoch 19/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3555 - loss: 2.8318\n",
            "Epoch 20/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4069 - loss: 2.7255\n",
            "Epoch 21/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4514 - loss: 2.5816\n",
            "Epoch 22/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4756 - loss: 2.4417\n",
            "Epoch 23/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5134 - loss: 2.3337\n",
            "Epoch 24/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5368 - loss: 2.2510\n",
            "Epoch 25/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6024 - loss: 2.1170\n",
            "Epoch 26/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6288 - loss: 1.9385\n",
            "Epoch 27/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6623 - loss: 1.8230\n",
            "Epoch 28/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6738 - loss: 1.7662\n",
            "Epoch 29/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7041 - loss: 1.6424\n",
            "Epoch 30/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7239 - loss: 1.5476\n",
            "Epoch 31/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7552 - loss: 1.4607\n",
            "Epoch 32/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7501 - loss: 1.4204\n",
            "Epoch 33/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7846 - loss: 1.3270\n",
            "Epoch 34/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8107 - loss: 1.1885\n",
            "Epoch 35/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8174 - loss: 1.1632\n",
            "Epoch 36/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8354 - loss: 1.0908\n",
            "Epoch 37/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8423 - loss: 1.0403\n",
            "Epoch 38/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8603 - loss: 0.9498\n",
            "Epoch 39/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8755 - loss: 0.9423\n",
            "Epoch 40/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8754 - loss: 0.8593\n",
            "Epoch 41/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8696 - loss: 0.8472\n",
            "Epoch 42/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8751 - loss: 0.7737\n",
            "Epoch 43/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8753 - loss: 0.7637\n",
            "Epoch 44/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8632 - loss: 0.7232\n",
            "Epoch 45/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8949 - loss: 0.6773\n",
            "Epoch 46/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8960 - loss: 0.6322\n",
            "Epoch 47/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8996 - loss: 0.6012\n",
            "Epoch 48/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9034 - loss: 0.5835\n",
            "Epoch 49/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9061 - loss: 0.5729\n",
            "Epoch 50/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8920 - loss: 0.5838\n",
            "Epoch 51/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9191 - loss: 0.5228\n",
            "Epoch 52/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8876 - loss: 0.5406\n",
            "Epoch 53/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8985 - loss: 0.5025\n",
            "Epoch 54/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9164 - loss: 0.4405\n",
            "Epoch 55/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9113 - loss: 0.4497\n",
            "Epoch 56/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9188 - loss: 0.4211\n",
            "Epoch 57/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8904 - loss: 0.4654\n",
            "Epoch 58/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8981 - loss: 0.4455\n",
            "Epoch 59/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9225 - loss: 0.3818\n",
            "Epoch 60/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8873 - loss: 0.4166\n",
            "Epoch 61/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9034 - loss: 0.3713\n",
            "Epoch 62/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9070 - loss: 0.3801\n",
            "Epoch 63/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9104 - loss: 0.3594\n",
            "Epoch 64/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.3669\n",
            "Epoch 65/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.3626\n",
            "Epoch 66/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9232 - loss: 0.3013\n",
            "Epoch 67/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9037 - loss: 0.3413\n",
            "Epoch 68/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9067 - loss: 0.3378\n",
            "Epoch 69/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9132 - loss: 0.3272\n",
            "Epoch 70/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9077 - loss: 0.3153\n",
            "Epoch 71/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9081 - loss: 0.3207\n",
            "Epoch 72/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9034 - loss: 0.3086\n",
            "Epoch 73/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9047 - loss: 0.3181\n",
            "Epoch 74/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8941 - loss: 0.3203\n",
            "Epoch 75/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9268 - loss: 0.2839\n",
            "Epoch 76/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8849 - loss: 0.3411\n",
            "Epoch 77/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8999 - loss: 0.3218\n",
            "Epoch 78/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9084 - loss: 0.2727\n",
            "Epoch 79/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9141 - loss: 0.2547\n",
            "Epoch 80/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9168 - loss: 0.2593\n",
            "Epoch 81/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8978 - loss: 0.2846\n",
            "Epoch 82/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9221 - loss: 0.2461\n",
            "Epoch 83/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9073 - loss: 0.2660\n",
            "Epoch 84/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8999 - loss: 0.2770\n",
            "Epoch 85/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9016 - loss: 0.2688\n",
            "Epoch 86/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8997 - loss: 0.2544\n",
            "Epoch 87/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9158 - loss: 0.2247\n",
            "Epoch 88/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9093 - loss: 0.2466\n",
            "Epoch 89/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9150 - loss: 0.2508\n",
            "Epoch 90/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8927 - loss: 0.2732\n",
            "Epoch 91/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8995 - loss: 0.2626\n",
            "Epoch 92/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9037 - loss: 0.2430\n",
            "Epoch 93/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9096 - loss: 0.2390\n",
            "Epoch 94/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9085 - loss: 0.2332\n",
            "Epoch 95/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9124 - loss: 0.2394\n",
            "Epoch 96/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9020 - loss: 0.2466\n",
            "Epoch 97/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8822 - loss: 0.2672\n",
            "Epoch 98/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9230 - loss: 0.2278\n",
            "Epoch 99/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9007 - loss: 0.2449\n",
            "Epoch 100/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9057 - loss: 0.2299\n",
            "Epoch 101/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9083 - loss: 0.2199\n",
            "Epoch 102/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9114 - loss: 0.2204\n",
            "Epoch 103/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8901 - loss: 0.2538\n",
            "Epoch 104/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9232 - loss: 0.2063\n",
            "Epoch 105/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9154 - loss: 0.2192\n",
            "Epoch 106/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9095 - loss: 0.2409\n",
            "Epoch 107/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9156 - loss: 0.2022\n",
            "Epoch 108/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9176 - loss: 0.1999\n",
            "Epoch 109/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9116 - loss: 0.2317\n",
            "Epoch 110/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9137 - loss: 0.2152\n",
            "Epoch 111/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9144 - loss: 0.1935\n",
            "Epoch 112/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9041 - loss: 0.2330\n",
            "Epoch 113/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9087 - loss: 0.2223\n",
            "Epoch 114/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9130 - loss: 0.2051\n",
            "Epoch 115/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9261 - loss: 0.1824\n",
            "Epoch 116/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8922 - loss: 0.2277\n",
            "Epoch 117/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9014 - loss: 0.2134\n",
            "Epoch 118/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9101 - loss: 0.1999\n",
            "Epoch 119/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8980 - loss: 0.2230\n",
            "Epoch 120/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9058 - loss: 0.2211\n",
            "Epoch 121/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9261 - loss: 0.1901\n",
            "Epoch 122/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9196 - loss: 0.2069\n",
            "Epoch 123/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9020 - loss: 0.2258\n",
            "Epoch 124/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9083 - loss: 0.2172\n",
            "Epoch 125/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8876 - loss: 0.2267\n",
            "Epoch 126/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8970 - loss: 0.2219\n",
            "Epoch 127/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8924 - loss: 0.2339\n",
            "Epoch 128/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9050 - loss: 0.1985\n",
            "Epoch 129/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9011 - loss: 0.2223\n",
            "Epoch 130/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9188 - loss: 0.1973\n",
            "Epoch 131/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.1923\n",
            "Epoch 132/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9003 - loss: 0.2161\n",
            "Epoch 133/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9140 - loss: 0.1938\n",
            "Epoch 134/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9089 - loss: 0.2015\n",
            "Epoch 135/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9128 - loss: 0.1954\n",
            "Epoch 136/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9168 - loss: 0.1928\n",
            "Epoch 137/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9085 - loss: 0.1977\n",
            "Epoch 138/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9148 - loss: 0.1847\n",
            "Epoch 139/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9005 - loss: 0.2145\n",
            "Epoch 140/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8947 - loss: 0.2070\n",
            "Epoch 141/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9127 - loss: 0.1864\n",
            "Epoch 142/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8949 - loss: 0.2309\n",
            "Epoch 143/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9078 - loss: 0.2144\n",
            "Epoch 144/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9095 - loss: 0.2064\n",
            "Epoch 145/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8941 - loss: 0.2207\n",
            "Epoch 146/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9050 - loss: 0.2175\n",
            "Epoch 147/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9176 - loss: 0.2049\n",
            "Epoch 148/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9060 - loss: 0.1986\n",
            "Epoch 149/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9052 - loss: 0.2130\n",
            "Epoch 150/150\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9079 - loss: 0.1976\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eb09c2fa850>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "\n",
        "# repeating the same steps we did for the preprocessing.\n",
        "\n",
        "# we will try to generate the next 10 words using a single word\n",
        "text = \"When\"\n",
        "result = \"\"\n",
        "\n",
        "for i in range(10):\n",
        "  sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "  sequence = pad_sequences([sequence], maxlen=max_len, padding='pre')\n",
        "  result_index = np.argmax(model.predict(sequence))\n",
        "\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == result_index:\n",
        "      text += \" \" + word\n",
        "      result += text\n",
        "      break\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "id": "qERkIMEUB6rM",
        "outputId": "8f24ed48-d6ec-4f3d-a065-b9eab06c341e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "When is the deadline for the project it’s due next friday\n"
          ]
        }
      ]
    }
  ]
}